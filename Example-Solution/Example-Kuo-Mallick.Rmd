---
output: github_document
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
```

Example solution
=================
The most direct approach to variable selection is to set the slab, \(\theta_j | (I_j = 1) \) equal to $\beta_j$ and spike $\theta_j | (I_j = 0)$ equal to zero. A simple example solution using the Kuo and Mallick method sets \(\theta_j = I_j \beta_j\) which assumes independence \(P(I_j, \beta_j) = P(I_j)P(\beta_j)\).

## Nimble package
For a quick solution, the nimble package is used to implement variable selection algorithms. The package allows statistical models to be written in BUGS form, built in R but compiles using C++ for speed. The package allows the user to implement their own samplers.

## Kuo-Mallick

Using the package nimble:
```{r, warning=F, message=F}
library(nimble)
dataset <- read.csv("../simulated_data.csv")
n <- 200; p <- 20
y <- dataset[,1]
x <- as.matrix(dataset[,-1])

# Set MCMC parameters
M.burnin <- 10000
M <- 5000
n.thin <- 5

myBUGScode <- nimbleCode({
  Pind <- 0.5
  alpha ~ dnorm(log(10),1)             # Intercept 
  tau ~ dgamma(1.0E-4,1.0E-4)          # precision param
  for(j in 1:p) {
    Ind[j] ~ dbern(Pind)               # Indicator
    beta[j] ~ dnorm(0,1)               # Conditional Regression coefficient
    theta[j] <- Ind[j]*beta[j]
  }
  for(i in 1:n) {
    er[i] ~ dnorm(0, tau)
    mu[i] <- alpha + inprod(theta[], X[i,]) + er[i]
    lambda[i] <- exp(mu[i])
    y[i] ~ dpois(lambda[i])          # Likelihood 
  }
})

```

For nimble, define the constants and dimensions, parse in model.

```{r, warning=F, message=F}
constants <- list(n = n, p=p)
dimensions = list(beta = p,
                  theta = p,
                  Ind =  p,
                  lambda =  n,
                  X = c(n,p),
                  er = n,
                  mu = n)
myModel <- nimbleModel(myBUGScode, 
                       constants = constants, 
                       dimensions = dimensions)
```

Give the model the data, initial values and build the MCMC.

```{r, warning=F, message=F}
myModel$setData(
  list(y = dataset[,1], X = as.matrix(dataset[,-1]))
  )

myModel$setInits(
  list(alpha=1, tau = 0.5, beta = matrix(0,nrow = 1, ncol = p), er = rnorm(n,0,1))
  )

myMCMC <- buildMCMC(myModel)
```

Using Nimble, compute the mcmc and plot the thinned samples:

```{r, warning=F, cache=T, message=F}
compiled <- compileNimble(myModel, myMCMC)

compiled$myMCMC$run(M.burnin + M*n.thin)
samples <- as.matrix(compiled$myMCMC$mvSamples)

thinnedsample <- samples[M.burnin + seq(from = 1,by = n.thin, 
                             to = M*n.thin),]
```

### Plots and analysis

Plot Density of coefficients for \(\theta_1\) values.

```{r, warning=F}
par(mfrow=c(2,2))
plot(density(thinnedsample[,"beta[1]"]*thinnedsample[,"Ind[1]"]), ylab = expression(theta), main = "Density for Theta[1]")
plot(density(thinnedsample[,"beta[1]"]), ylab = expression(beta), main = "Density for Beta[1]")
plot(density(thinnedsample[,"Ind[1]"]), ylab = expression(theta), main = "Density for Ind[1]")
plot(samples[-c(1:M.burnin),"beta[1]"],  type = 'l', xlab = 'iteration',  ylab = expression(beta), main = "trace for Beta[1]")
par(mfrow=c(1,1))
```

The \(\theta_1\) parameter is strongly concentrated at zero, trace plots for beta show convergence for the beta parameter and density for \(I_1\) also indicate evidence that this variable does not contribute much to the regression. The true value for \(\theta_1 = 0.02857143\) which is very small, the posterior mean estimate is `r round(mean(thinnedsample[,"beta[1]"]*thinnedsample[,"Ind[1]"]),6)`.

Traceplots show some potential issues for parameters, eg \(\theta_{12}\) with trace plot:

```{r}
plot(samples[-c(1:M.burnin),"beta[12]"],  type = 'l', xlab = 'iteration',  ylab = expression(beta), main = "trace for Beta[12]")
```

Using the skimr package to see posterior summary results:

```{r, echo=F}
library(skimr)
skim_format(numeric = list(digits = 3))
res <- data.frame(thinnedsample[,1:20]*thinnedsample[,22:41])
res <- cbind(res, thinnedsample[,1:20])
colnames(res) <- c(paste0("beta_0",1:9),paste0("beta_",10:20),paste0("Ind_0",1:9), paste0("Ind_",10:20))

skim_with(numeric = list(missing=NULL, complete=NULL))

res %>%
  skim(.) %>%
  kable(.)
```

## Limits for the Kuo-Mallick method

+ Results for the variable selection are not stable using the  Kuo and Mallick method (also reported in Oâ€™Hara and Sillanpaa). Need a better method for variable selection.
+ Very simple method but appears to perform poorly on this dataset.
+ Additional graphical exploration would provide further insight.


